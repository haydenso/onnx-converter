# Summary: New ONNX Converter with Qwen3 Support

## What You Asked For

You needed a new converter script that:
1. Works with **Qwen3** models (not supported in old Transformers.js approach)
2. Uses the modern **ONNX Runtime GenAI builder** from Microsoft
3. Replaces the old app.py that only supported models up to Qwen2

## What I Created

### üìÅ New Files

1. **app.py** - Main Streamlit application using ONNX Runtime GenAI builder
2. **requirements.txt** - Updated dependencies including onnxruntime-genai
3. **README.md** - Complete documentation with examples and troubleshooting
4. **setup.sh** - Quick setup script to install dependencies
5. **test_setup.py** - Test script to verify everything is installed correctly

### üÜö Key Differences from Old Version

| Feature | Old (app.py) | New (app.py) |
|---------|-------------|------------------|
| **Converter** | Transformers.js scripts | ONNX Runtime GenAI builder |
| **Qwen Support** | Only Qwen2 | ‚úÖ **Qwen3** + Qwen2 |
| **Latest Models** | ‚ùå No Gemma3, Phi4, SmolLM3 | ‚úÖ All latest models |
| **Dependencies** | Bundled transformers.js repo | Direct pip install |
| **Quantization** | Basic | Advanced INT4 options |
| **Architecture Check** | Manual | ‚úÖ Automatic validation |

### ‚ú® Supported Model Families (26 architectures!)

- **Qwen3** ‚Üê Your main goal! ‚ú®
- Qwen2, Qwen2.5-VL
- Gemma, Gemma 2, Gemma 3 (text & multimodal)
- Phi, Phi-3, Phi-3 Vision, Phi-3 MoE, Phi-4
- SmolLM3
- Llama, Mistral
- ChatGLM, Granite, Nemotron, OLMo, Ernie, GPT-OSS

## üöÄ Quick Start

### Step 1: Install Dependencies

```bash
# Option A: Use the setup script
./setup.sh

# Option B: Manual install
pip install -r requirements.txt
```

### Step 2: Test Your Setup

```bash
python test_setup.py
```

This will verify:
- All dependencies are installed
- ONNX Runtime GenAI builder is accessible
- Qwen3 is in the supported models list

### Step 3: Run the Converter

```bash
streamlit run app.py
```

### Step 4: Convert a Qwen3 Model

1. Enter model ID: `Qwen/Qwen3-0.5B-Instruct`
2. Select precision: `fp16` (or `int4` for quantized)
3. Select execution provider: `cuda` (or `cpu`)
4. Click "Start Conversion"

The app will:
- ‚úÖ Validate the model is compatible
- ‚úÖ Download and convert to ONNX
- ‚úÖ Generate a README
- ‚úÖ Upload to your Hugging Face account

## üìã Usage Examples

### Example 1: Basic Qwen3 Conversion (FP16, CUDA)

```bash
# Via Streamlit UI
streamlit run app.py
# Then enter: Qwen/Qwen3-0.5B-Instruct

# Or via command line (if builder is installed)
python -m onnxruntime_genai.models.builder \
  -m Qwen/Qwen3-0.5B-Instruct \
  -o ./qwen3_fp16 \
  -p fp16 \
  -e cuda
```

### Example 2: Quantized Qwen3 (INT4, CUDA)

```bash
# Via command line
python -m onnxruntime_genai.models.builder \
  -m Qwen/Qwen3-0.5B-Instruct \
  -o ./qwen3_int4 \
  -p int4 \
  -e cuda \
  --extra_options int4_block_size=32 int4_is_symmetric=true
```

### Example 3: CPU Inference (FP32)

```bash
python -m onnxruntime_genai.models.builder \
  -m Qwen/Qwen3-0.5B-Instruct \
  -o ./qwen3_cpu \
  -p fp32 \
  -e cpu
```

## üîß How It Works

### Architecture

```
app.py
    ‚Üì
[Import onnxruntime_genai.models.builder]
    ‚Üì
create_model() function
    ‚Üì
[Download model from HF] ‚Üí [Convert to ONNX] ‚Üí [Save files]
    ‚Üì
[Generate README] ‚Üí [Upload to HF]
```

### Key Components

1. **ModelConverter Class**
   - Checks model compatibility
   - Calls the builder's `create_model()` function
   - Generates README with model card
   - Uploads to Hugging Face

2. **Builder Integration**
   - Imports from `onnxruntime_genai.models.builder`
   - Supports all architectures in the builder
   - Passes options like precision, execution provider, quantization settings

3. **Streamlit UI**
   - User-friendly interface
   - Model compatibility checker
   - Advanced options for INT4 quantization
   - Progress indicators and error handling

## üéØ What's Better in the New Version?

### 1. **Automatic Model Validation**
Old version: ‚ùå Would fail during conversion
New version: ‚úÖ Checks compatibility upfront and shows supported architectures

### 2. **Modern Architecture Support**
Old version: ‚ùå Only Qwen2
New version: ‚úÖ Qwen3, Gemma3, Phi4, SmolLM3, and more

### 3. **Better User Experience**
- Real-time compatibility checking
- Clear error messages with suggestions
- Advanced options hidden in expander (cleaner UI)
- Better progress indicators

### 4. **Advanced Quantization**
- INT4 block size options (16, 32, 64, 128, 256)
- Symmetric vs asymmetric quantization
- Accuracy level control (int8, bf16, fp16, fp32)

### 5. **Direct Integration**
Old version: Required bundled transformers.js repo
New version: Direct pip install, no bundled repos needed

## üìù Migration Notes

If you have existing code using the old app.py:

### Import Changes
```python
# Old
from scripts.convert import convert_model  # Transformers.js

# New  
from onnxruntime_genai.models.builder import create_model
```

### API Changes
```python
# Old (Transformers.js)
subprocess.run([
    "python", "-m", "scripts.convert",
    "--quantize",
    "--model_id", model_id,
    "--task", task
])

# New (ONNX Runtime GenAI)
create_model(
    model_name=model_id,
    input_path="",
    output_dir=output_dir,
    precision="int4",  # Instead of --quantize
    execution_provider="cuda",
    cache_dir=cache_dir,
)
```

## ‚ö†Ô∏è Important Notes

### Gemma Models
Gemma 2 and Gemma 3 lose accuracy with FP16. The converter will warn you to use:
- `--precision bf16` OR
- `--precision int4 --extra_options use_cuda_bf16=true`

### Multimodal Models
Models like Phi-3 Vision, Phi-4, Gemma3 multimodal only export the text component.
The converter automatically sets `exclude_embeds=true`.

### CUDA Graphs
Only works if all nodes are on CUDA EP. Not guaranteed to work for all models.

## üêõ Troubleshooting

### "Cannot find onnxruntime-genai builder"
```bash
pip install onnxruntime-genai
```

### "Model architecture not supported"
Check the model's config.json on Hugging Face. If the architecture isn't in SUPPORTED_ARCHITECTURES dict in app.py, it's not supported yet.

### CUDA Out of Memory
Try INT4 quantization:
```bash
--precision int4
```

### Installation Issues
If `pip install onnxruntime-genai` fails, try installing from source:
```bash
git clone https://github.com/microsoft/onnxruntime-genai.git
cd onnxruntime-genai/src/python
pip install -e .
```

## üìö Resources

- **ONNX Runtime GenAI**: https://github.com/microsoft/onnxruntime-genai
- **Builder Source**: https://github.com/microsoft/onnxruntime-genai/blob/main/src/python/py/models/builder.py
- **Documentation**: https://onnxruntime.ai/docs/genai/

## ‚úÖ Next Steps

1. ‚úÖ Install dependencies: `pip install -r requirements.txt`
2. ‚úÖ Test setup: `python test_setup.py`
3. ‚úÖ Run converter: `streamlit run app.py`
4. ‚úÖ Convert your Qwen3 model!

---

**You now have a fully functional ONNX converter that supports Qwen3 and all the latest models!** üéâ
