# Quick Preset Reference Card

## üéØ Choose Your Preset in 3 Questions

### Question 1: What hardware are you targeting?
- **NVIDIA GPU (RTX 30xx or newer)** ‚Üí Go to Q2
- **NVIDIA GPU (older)** ‚Üí **FP16 - Recommended**
- **CPU** ‚Üí **FP32 - Full Precision**
- **Mobile/Edge device** ‚Üí **INT4 - 4-bit Quantized**

### Question 2: What model family?
- **Gemma (any version)** ‚Üí **BF16 - Brain Float**
- **Phi (any version)** ‚Üí **BF16 - Brain Float**
- **Qwen, Llama, Mistral, other** ‚Üí **FP16 - Recommended**

### Question 3: Need to save space?
- **Yes, smaller file important** ‚Üí See Q4
- **No, quality matters most** ‚Üí Use answer from Q2

### Question 4: Quantization (size reduction)
- **Gemma/Phi + need small size** ‚Üí **INT4 + BF16 Activations**
- **Other models + need small size** ‚Üí **INT4 + FP16 Activations**
- **Mobile/Edge + smallest possible** ‚Üí **INT4 - 4-bit Quantized**

---

## üìä Preset Comparison Table

| Preset | Size | Quality | Speed | Hardware |
|--------|------|---------|-------|----------|
| FP32 - Full Precision | 100% | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | CPU |
| FP16 - Recommended | 50% | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | GPU ‚≠ê |
| BF16 - Brain Float | 50% | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | GPU (Gemma/Phi) |
| INT4 - 4-bit | 25% | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Mobile |
| INT4 + INT8 | 27% | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | GPU/CPU |
| INT4 + BF16 | 27% | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | GPU (Gemma/Phi) |
| INT4 + FP16 | 27% | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | GPU |
| UINT4 - Asymmetric | 25% | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | Alternative |

Size = relative to FP32 baseline

---

## üî• Most Common Use Cases

### 1. "I just want to convert a model for my NVIDIA GPU"
‚Üí **FP16 - Recommended (GPU)** ‚≠ê

### 2. "I'm converting a Gemma or Phi model"
‚Üí **BF16 - Brain Float (Gemma/Phi)**

### 3. "I need the smallest possible file for mobile"
‚Üí **INT4 - 4-bit Quantized**

### 4. "I want to run on CPU with best quality"
‚Üí **FP32 - Full Precision (CPU)**

### 5. "I want Gemma but smaller file size"
‚Üí **INT4 + BF16 Activations**

### 6. "I want good quality but smaller than FP16"
‚Üí **INT4 + FP16 Activations**

---

## üéì Model-Specific Recommendations

### Qwen3
- **Best**: FP16 - Recommended
- **Small**: INT4 + FP16 Activations
- **CPU**: FP32 - Full Precision

### Gemma (all versions)
- **Best**: BF16 - Brain Float ‚≠ê
- **Small**: INT4 + BF16 Activations
- **CPU**: FP32 - Full Precision

### Phi-3 / Phi-4
- **Best**: BF16 - Brain Float ‚≠ê
- **Small**: INT4 + BF16 Activations
- **CPU**: FP32 - Full Precision

### SmolLM3
- **Best**: FP16 - Recommended
- **Small**: INT4 - 4-bit Quantized ‚≠ê
- **CPU**: FP32 - Full Precision

### Llama / Mistral
- **Best**: FP16 - Recommended
- **Small**: INT4 + FP16 Activations
- **CPU**: FP32 - Full Precision

---

## ‚ö° Performance Tips

### Speed Priority
1. INT4 - 4-bit Quantized (fastest)
2. INT4 + FP16 Activations
3. FP16 - Recommended
4. BF16 - Brain Float
5. FP32 - Full Precision (slowest)

### Quality Priority
1. FP32 - Full Precision (best)
2. BF16 - Brain Float (for Gemma/Phi)
3. FP16 - Recommended
4. INT4 + BF16 Activations
5. INT4 + FP16 Activations
6. INT4 + INT8 Activations
7. INT4 - 4-bit Quantized (lowest)

### Size Priority
1. INT4 - 4-bit Quantized (smallest)
2. UINT4 - Asymmetric
3. INT4 + FP16 Activations
4. INT4 + BF16 Activations
5. INT4 + INT8 Activations
6. FP16 - Recommended
7. BF16 - Brain Float
8. FP32 - Full Precision (largest)

---

## üö® Common Mistakes to Avoid

‚ùå **Using FP16 for Gemma/Phi**
‚Üí Use BF16 instead for better accuracy

‚ùå **Using FP32 for GPU inference**
‚Üí Use FP16 or BF16 instead (2x smaller, same quality)

‚ùå **Using INT4 without understanding quality loss**
‚Üí Test quality before deploying to production

‚ùå **Not matching precision to hardware**
‚Üí FP16/BF16 for GPU, FP32 for CPU, INT4 for mobile

‚ùå **Using asymmetric quantization by default**
‚Üí Stick with symmetric (default) unless you have a reason

---

## üì± Hardware-Specific Guide

### RTX 3090 / 4090
- **Best**: FP16 - Recommended
- **Gemma/Phi**: BF16 - Brain Float
- **Save VRAM**: INT4 + FP16 Activations

### A100 / H100
- **Best**: BF16 - Brain Float ‚≠ê
- **Alternative**: FP16 - Recommended
- **Save VRAM**: INT4 + BF16 Activations

### CPU (Intel/AMD)
- **Best**: FP32 - Full Precision
- **Faster**: INT4 + INT8 Activations
- **Smallest**: INT4 - 4-bit Quantized

### Mobile (Android/iOS)
- **Best**: INT4 - 4-bit Quantized ‚≠ê
- **Better quality**: INT4 + FP16 Activations
- **Highest quality**: FP16 - Recommended (if VRAM allows)

### Jetson Nano / Edge Devices
- **Best**: INT4 - 4-bit Quantized ‚≠ê
- **Alternative**: INT4 + INT8 Activations

---

## üîç Troubleshooting by Symptom

### "Model is too big"
‚Üí Try: INT4 - 4-bit Quantized

### "Quality is bad after conversion"
‚Üí Try: Move up the quality ladder (INT4‚ÜíFP16‚ÜíFP32)

### "Gemma model has bad accuracy"
‚Üí Try: BF16 - Brain Float ‚≠ê

### "Out of VRAM during inference"
‚Üí Try: INT4 + FP16 Activations

### "Out of memory during conversion"
‚Üí Try: Use CPU execution provider temporarily

### "Conversion is too slow"
‚Üí This is normal, especially for large models

---

## üì• Output Files

**All presets produce**: `model.onnx` (or `decoder_model.onnx`)

The precision is embedded in the file, not the filename. 

To create multiple versions:
1. Convert with preset A ‚Üí rename to `model_a.onnx`
2. Convert with preset B ‚Üí rename to `model_b.onnx`
3. etc.

---

## üÜò When to Use "Custom"

Use **Custom - Manual Configuration** when:
- You know exact settings you need
- Experimenting with different configurations
- Following specific model author recommendations
- Debugging conversion issues
- Creating non-standard configurations

For 95% of users, presets are better.

---

**See PRESETS_GUIDE.md for complete documentation**
